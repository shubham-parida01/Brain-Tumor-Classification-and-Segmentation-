MODEL ARCHITECTURE
-----------------
1. Base Architecture
- ResNet50 variant: ResNet50-1x (standard configuration)
- Pre-trained on ImageNet
- Modified final layers for 4-class classification
- Input size: 224x224x3

2. Multi-Task Architecture
The model splits into two branches after the ResNet50 backbone:

Classification Branch:
- Global Average Pooling layer
- Fully Connected layer (2048 -> 1024)
- Dropout (0.5)
- Final classification layer (1024 -> 4)

Segmentation Branch:
- Feature Pyramid Network (FPN) decoder
- Skip connections from ResNet stages
- Upsampling layers (2x, 4x, 8x)
- Final segmentation head (1x1 conv)

3. Attention Mechanisms
Channel Attention Module:
```python
class ChannelAttention(nn.Module):
    def __init__(self, in_channels, reduction_ratio=16):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(in_channels, in_channels // reduction_ratio),
            nn.ReLU(),
            nn.Linear(in_channels // reduction_ratio, in_channels)
        )
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc(self.avg_pool(x).view(x.size(0), -1))
        max_out = self.fc(self.max_pool(x).view(x.size(0), -1))
        out = avg_out + max_out
        return self.sigmoid(out).view(x.size(0), x.size(1), 1, 1) * x
```

Spatial Attention Module:
```python
class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super().__init__()
        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avg_out, max_out], dim=1)
        x = self.conv(x)
        return self.sigmoid(x) * x
```

4. Multi-Task Loss
```python
class MultiTaskLoss(nn.Module):
    def __init__(self, alpha=0.7):
        super().__init__()
        self.alpha = alpha
        self.ce_loss = nn.CrossEntropyLoss()
        self.dice_loss = DiceLoss()

    def forward(self, pred_cls, pred_seg, target_cls, target_seg):
        cls_loss = self.ce_loss(pred_cls, target_cls)
        seg_loss = self.dice_loss(pred_seg, target_seg)
        return self.alpha * cls_loss + (1 - self.alpha) * seg_loss
```

TRAINING CONFIGURATION
--------------------
1. Optimizer Settings
- Optimizer: AdamW
- Learning Rate: 0.001
- Weight Decay: 0.0001
- Beta1: 0.9
- Beta2: 0.999
- Epsilon: 1e-8

2. Learning Rate Schedule
- Initial LR: 0.001
- Final LR: 0.00001
- Warmup epochs: 5
- Cosine annealing schedule
- Cycle length: 100 epochs

3. Training Parameters
- Batch Size: 32
- Total Epochs: 100
- Early Stopping: Patience = 10
- Gradient Clipping: 1.0

4. Loss Functions
Classification:
- Cross Entropy Loss
- Label Smoothing: 0.1

Segmentation:
- Dice Loss
- Binary Cross Entropy
- Combined loss: 0.5 * Dice + 0.5 * BCE

5. Data Augmentation
```python
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                       std=[0.229, 0.224, 0.225])
])
```

GRAD-CAM IMPLEMENTATION
----------------------
```python
class GradCAM:
    def __init__(self, model, target_layer):
        self.model = model
        self.target_layer = target_layer
        self.gradients = None
        self.activations = None
        
        target_layer.register_forward_hook(self.save_activation)
        target_layer.register_backward_hook(self.save_gradient)

    def save_activation(self, module, input, output):
        self.activations = output

    def save_gradient(self, module, grad_input, grad_output):
        self.gradients = grad_output[0]

    def __call__(self, input_tensor, target_class=None):
        self.model.zero_grad()
        output = self.model(input_tensor)
        
        if target_class is None:
            target_class = output.argmax(dim=1)
            
        one_hot = torch.zeros_like(output)
        one_hot[0][target_class] = 1
        
        output.backward(gradient=one_hot, retain_graph=True)
        
        gradients = self.gradients.detach()
        activations = self.activations.detach()
        
        weights = torch.mean(gradients, dim=(2, 3), keepdim=True)
        cam = torch.sum(weights * activations, dim=1, keepdim=True)
        cam = F.relu(cam)
        cam = F.interpolate(cam, size=input_tensor.shape[2:], mode='bilinear', align_corners=False)
        cam = cam - cam.min()
        cam = cam / cam.max()
        
        return cam
```

COMPARISON WITH BASELINE MODELS
-----------------------------
| Model | Accuracy | Precision | Recall | F1-Score | Parameters |
|-------|----------|-----------|--------|----------|------------|
| Our Model | 92.68% | 92.76% | 92.32% | 92.35% | 25.5M |
| ResNet50 | 89.45% | 89.32% | 89.12% | 89.22% | 25.5M |
| VGG16 | 87.23% | 87.12% | 87.01% | 87.06% | 138M |
| UNet | 85.67% | 85.45% | 85.23% | 85.34% | 31.0M |
| EfficientNet-B0 | 88.92% | 88.78% | 88.65% | 88.71% | 5.3M |

DATASET SPECIFICATIONS
---------------------
1. Dataset Composition
- Total Images: 2,870
- Training: 2,009 (70%)
- Validation: 431 (15%)
- Test: 430 (15%)

2. Class Distribution
- Glioma: 826 images
- Meningioma: 822 images
- Pituitary: 827 images
- No Tumor: 395 images

3. Image Specifications
- Format: JPEG
- Resolution: 224x224 (resized)
- Channels: RGB
- Modalities: T1-weighted MRI
- Source: Multiple medical centers
- Annotations: Expert radiologist verified

4. Preprocessing
- Normalization: ImageNet statistics
- Augmentation: See Data Augmentation section
- Quality Control: Manual review of all images
- Exclusion Criteria: Poor quality, artifacts, incomplete scans 