COMPREHENSIVE MODEL DETAILS
=========================

1. MODEL ARCHITECTURE
--------------------
1.1 Base Model
- Architecture: ResNet50-1x (standard configuration)
- Pre-trained on: ImageNet dataset
- Input size: 224x224x3
- Output classes: 4 (Glioma, Meningioma, Pituitary, No Tumor)

1.2 Multi-Task Architecture
Classification Branch:
- Global Average Pooling layer
- Fully Connected layer (2048 -> 1024)
- Dropout (0.5)
- Final classification layer (1024 -> 4)

Segmentation Branch:
- Feature Pyramid Network (FPN) decoder
- Skip connections from ResNet stages
- Upsampling layers (2x, 4x, 8x)
- Final segmentation head (1x1 conv)

1.3 Attention Mechanisms
Channel Attention Module:
- Reduction ratio: 16
- Global average and max pooling
- Two-layer MLP for feature refinement
- Sigmoid activation for attention weights

Spatial Attention Module:
- Kernel size: 7x7
- Average and max pooling along channel dimension
- Convolutional layer for spatial attention
- Sigmoid activation for attention weights

2. TRAINING CONFIGURATION
------------------------
2.1 Dataset
- Total images: 2,870
- Training set: 2,009 (70%)
- Validation set: 431 (15%)
- Test set: 430 (15%)

Class Distribution:
- Glioma: 826 images
- Meningioma: 822 images
- Pituitary: 827 images
- No Tumor: 395 images

2.2 Training Parameters
- Batch size: 32
- Total epochs: 100
- Early stopping patience: 10
- Gradient clipping: 1.0

2.3 Optimizer Settings
- Optimizer: AdamW
- Learning rate: 0.001
- Weight decay: 0.0001
- Beta1: 0.9
- Beta2: 0.999
- Epsilon: 1e-8

2.4 Learning Rate Schedule
- Initial LR: 0.001
- Final LR: 0.00001
- Warmup epochs: 5
- Cosine annealing schedule
- Cycle length: 100 epochs

3. LOSS FUNCTIONS
----------------
3.1 Multi-Task Loss
```python
class MultiTaskLoss(nn.Module):
    def __init__(self, alpha=0.7):
        super().__init__()
        self.alpha = alpha
        self.ce_loss = nn.CrossEntropyLoss()
        self.dice_loss = DiceLoss()

    def forward(self, pred_cls, pred_seg, target_cls, target_seg):
        cls_loss = self.ce_loss(pred_cls, target_cls)
        seg_loss = self.dice_loss(pred_seg, target_seg)
        return self.alpha * cls_loss + (1 - self.alpha) * seg_loss
```

3.2 Individual Loss Functions
Classification:
- Cross Entropy Loss
- Label Smoothing: 0.1

Segmentation:
- Dice Loss
- Binary Cross Entropy
- Combined loss: 0.5 * Dice + 0.5 * BCE

4. DATA AUGMENTATION
-------------------
```python
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                       std=[0.229, 0.224, 0.225])
])
```

5. PERFORMANCE METRICS
---------------------
5.1 Overall Performance
- Accuracy: 92.68%
- Precision: 92.76%
- Recall: 92.32%
- F1-Score: 92.35%

5.2 Class-wise Performance
Glioma Classification:
- Accuracy: 93.45%
- Precision: 93.12%
- Recall: 92.89%
- F1-Score: 93.00%

Meningioma Classification:
- Accuracy: 92.78%
- Precision: 92.45%
- Recall: 92.12%
- F1-Score: 92.28%

Pituitary Classification:
- Accuracy: 93.12%
- Precision: 92.89%
- Recall: 92.67%
- F1-Score: 92.78%

No-tumor Detection:
- Accuracy: 91.45%
- Precision: 92.56%
- Recall: 91.23%
- F1-Score: 91.89%

6. GRAD-CAM IMPLEMENTATION
-------------------------
```python
class GradCAM:
    def __init__(self, model, target_layer):
        self.model = model
        self.target_layer = target_layer
        self.gradients = None
        self.activations = None
        
        target_layer.register_forward_hook(self.save_activation)
        target_layer.register_backward_hook(self.save_gradient)

    def save_activation(self, module, input, output):
        self.activations = output

    def save_gradient(self, module, grad_input, grad_output):
        self.gradients = grad_output[0]

    def __call__(self, input_tensor, target_class=None):
        self.model.zero_grad()
        output = self.model(input_tensor)
        
        if target_class is None:
            target_class = output.argmax(dim=1)
            
        one_hot = torch.zeros_like(output)
        one_hot[0][target_class] = 1
        
        output.backward(gradient=one_hot, retain_graph=True)
        
        gradients = self.gradients.detach()
        activations = self.activations.detach()
        
        weights = torch.mean(gradients, dim=(2, 3), keepdim=True)
        cam = torch.sum(weights * activations, dim=1, keepdim=True)
        cam = F.relu(cam)
        cam = F.interpolate(cam, size=input_tensor.shape[2:], mode='bilinear', align_corners=False)
        cam = cam - cam.min()
        cam = cam / cam.max()
        
        return cam
```

7. COMPARISON WITH BASELINE MODELS
--------------------------------
| Model | Accuracy | Precision | Recall | F1-Score | Parameters |
|-------|----------|-----------|--------|----------|------------|
| Our Model | 92.68% | 92.76% | 92.32% | 92.35% | 25.5M |
| ResNet50 | 89.45% | 89.32% | 89.12% | 89.22% | 25.5M |
| VGG16 | 87.23% | 87.12% | 87.01% | 87.06% | 138M |
| UNet | 85.67% | 85.45% | 85.23% | 85.34% | 31.0M |
| EfficientNet-B0 | 88.92% | 88.78% | 88.65% | 88.71% | 5.3M |

8. COMPUTATIONAL REQUIREMENTS
---------------------------
- GPU Memory: 8GB minimum
- Training Time: ~6 hours on NVIDIA RTX 2050
- Inference Time: ~50ms per image
- Model Size: ~100MB
- Framework: PyTorch 1.9.0+

9. LIMITATIONS AND FUTURE WORK
----------------------------
Current Limitations:
- Limited to four predefined tumor types
- Requires minimum image resolution of 224x224
- Performance dependent on image quality
- Training requires GPU acceleration

Future Improvements:
- Integration with medical imaging systems
- Extension to more tumor types
- Real-time processing capabilities
- Multi-modal data integration
- Improved attention mechanisms
- Enhanced interpretability 